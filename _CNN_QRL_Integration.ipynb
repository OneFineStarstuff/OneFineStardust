{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO2bqoVYt0sb7aMf6lHJ0NJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/_CNN_QRL_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna pennylane"
      ],
      "metadata": {
        "id": "q3zOB_6sSeDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3hcDEnVSAM0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms import functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import random\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "# Placeholder for loading dataset\n",
        "train_images = torch.randn(100, 3, 64, 64)  # Example: 100 images, 3 channels, 64x64 resolution\n",
        "train_labels = torch.randint(0, 5, (100,))  # Example: 100 labels, 5 classes\n",
        "test_images = torch.randn(20, 3, 64, 64)   # Example: 20 test images\n",
        "test_labels = torch.randint(0, 5, (20,))   # Example: 20 test labels\n",
        "\n",
        "# Custom transform function\n",
        "def tensor_transform(image):\n",
        "    image = F.rotate(image, angle=30)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.hflip(image)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.vflip(image)\n",
        "    return F.resized_crop(image, top=0, left=0, height=64, width=64, size=(64, 64))\n",
        "\n",
        "# Custom Dataset class\n",
        "class GalaxyDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Improved CNN with Pre-trained Model\n",
        "class GalaxyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GalaxyCNN, self).__init__()\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)  # Assuming 5 classes\n",
        "        for param in self.resnet.parameters():  # Freeze layers\n",
        "            param.requires_grad = False\n",
        "        for param in self.resnet.fc.parameters():  # Train only the final layers\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Quantum Policy for Reinforcement Learning\n",
        "n_qubits = 4\n",
        "n_layers = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "class CriticNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(CriticNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "# Hyperparameter tuning with Optuna\n",
        "def objective(trial):\n",
        "    actor_lr = trial.suggest_float('actor_lr', 1e-5, 1e-2, log=True)\n",
        "    critic_lr = trial.suggest_float('critic_lr', 1e-5, 1e-2, log=True)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "    n_epochs = 20\n",
        "    gamma = 0.99\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rewards, log_probs, state_values = [], [], []\n",
        "        for _ in range(10):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            value = critic(state).squeeze()\n",
        "            state_values.append(value)\n",
        "            log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "            rewards.append(reward)\n",
        "\n",
        "        discounted_rewards = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "        policy_loss, value_loss = [], []\n",
        "        for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "            value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "        total_loss += torch.stack(policy_loss).sum().item() + torch.stack(value_loss).sum().item()\n",
        "\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        value_loss = torch.stack(value_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        value_loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    return total_loss / n_epochs\n",
        "\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best Hyperparameters:\", study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms import functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import random\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "# Placeholder for loading dataset\n",
        "train_images = torch.randn(100, 3, 64, 64)  # Example: 100 images, 3 channels, 64x64 resolution\n",
        "train_labels = torch.randint(0, 5, (100,))  # Example: 100 labels, 5 classes\n",
        "test_images = torch.randn(20, 3, 64, 64)   # Example: 20 test images\n",
        "test_labels = torch.randint(0, 5, (20,))   # Example: 20 test labels\n",
        "\n",
        "# Custom transform function\n",
        "def tensor_transform(image):\n",
        "    image = F.rotate(image, angle=30)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.hflip(image)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.vflip(image)\n",
        "    return F.resized_crop(image, top=0, left=0, height=64, width=64, size=(64, 64))\n",
        "\n",
        "# Custom Dataset class\n",
        "class GalaxyDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Improved CNN with Pre-trained Model\n",
        "class GalaxyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GalaxyCNN, self).__init__()\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)  # Assuming 5 classes\n",
        "        for param in self.resnet.parameters():  # Freeze layers\n",
        "            param.requires_grad = False\n",
        "        for param in self.resnet.fc.parameters():  # Train only the final layers\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Quantum Policy for Reinforcement Learning\n",
        "n_qubits = 4\n",
        "n_layers = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "class CriticNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(CriticNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "# Hyperparameter tuning with Optuna\n",
        "def objective(trial):\n",
        "    actor_lr = trial.suggest_float('actor_lr', 1e-5, 1e-2, log=True)\n",
        "    critic_lr = trial.suggest_float('critic_lr', 1e-5, 1e-2, log=True)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "    n_epochs = 20\n",
        "    gamma = 0.99\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rewards, log_probs, state_values = [], [], []\n",
        "        for _ in range(10):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            value = critic(state).squeeze()\n",
        "            state_values.append(value)\n",
        "            log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "            rewards.append(reward)\n",
        "\n",
        "        discounted_rewards = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "        policy_loss, value_loss = [], []\n",
        "        for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "            value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "        total_loss += torch.stack(policy_loss).sum().item() + torch.stack(value_loss).sum().item()\n",
        "\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        value_loss = torch.stack(value_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        value_loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    return total_loss / n_epochs\n",
        "\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "best_actor_lr = study.best_params['actor_lr']\n",
        "best_critic_lr = study.best_params['critic_lr']\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=best_actor_lr)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=best_critic_lr)"
      ],
      "metadata": {
        "id": "H5iLn2a6VKuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "buXprCYlWbVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms import functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import random\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "# Placeholder for loading dataset\n",
        "train_images = torch.randn(100, 3, 64, 64)  # Example: 100 images, 3 channels, 64x64 resolution\n",
        "train_labels = torch.randint(0, 5, (100,))  # Example: 100 labels, 5 classes\n",
        "test_images = torch.randn(20, 3, 64, 64)   # Example: 20 test images\n",
        "test_labels = torch.randint(0, 5, (20,))   # Example: 20 test labels\n",
        "\n",
        "# Custom transform function\n",
        "def tensor_transform(image):\n",
        "    image = F.rotate(image, angle=30)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.hflip(image)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.vflip(image)\n",
        "    return F.resized_crop(image, top=0, left=0, height=64, width=64, size=(64, 64))\n",
        "\n",
        "# Custom Dataset class\n",
        "class GalaxyDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Improved CNN with Pre-trained Model\n",
        "class GalaxyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GalaxyCNN, self).__init__()\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)  # Assuming 5 classes\n",
        "        for param in self.resnet.parameters():  # Freeze layers\n",
        "            param.requires_grad = False\n",
        "        for param in self.resnet.fc.parameters():  # Train only the final layers\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Quantum Policy for Reinforcement Learning\n",
        "n_qubits = 4\n",
        "n_layers = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "class CriticNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(CriticNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "# Hyperparameter tuning with Optuna\n",
        "def objective(trial):\n",
        "    actor_lr = trial.suggest_float('actor_lr', 1e-5, 1e-2, log=True)\n",
        "    critic_lr = trial.suggest_float('critic_lr', 1e-5, 1e-2, log=True)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "    n_epochs = 20\n",
        "    gamma = 0.99\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rewards, log_probs, state_values = [], [], []\n",
        "        for _ in range(10):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            value = critic(state).squeeze()\n",
        "            state_values.append(value)\n",
        "            log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "            rewards.append(reward)\n",
        "\n",
        "        discounted_rewards = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "        policy_loss, value_loss = [], []\n",
        "        for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "            value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "        total_loss += torch.stack(policy_loss).sum().item() + torch.stack(value_loss).sum().item()\n",
        "\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        value_loss = torch.stack(value_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        value_loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    return total_loss / n_epochs\n",
        "\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "best_actor_lr = study.best_params['actor_lr']\n",
        "best_critic_lr = study.best_params['critic_lr']\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=best_actor_lr)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=best_critic_lr)\n",
        "\n",
        "# Further Training with Best Hyperparameters\n",
        "n_epochs = 50  # Increase the number of epochs for final training\n",
        "gamma = 0.99\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    rewards, log_probs, state_values = [], [], []\n",
        "    for _ in range(10):\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "        policy_output = actor(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "        reward = random.uniform(-1, 1)\n",
        "        value = critic(state).squeeze()\n",
        "        state_values.append(value)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "        rewards.append(reward)\n",
        "\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    policy_loss, value_loss = [], []\n",
        "    for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "        advantage = reward - value.item()\n",
        "        policy_loss.append(-log_prob * advantage)\n",
        "        value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    value_loss = torch.stack(value_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    value_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} - Policy Loss: {policy_loss.item()} - Value Loss: {value_loss.item()}\")\n",
        "\n",
        "# Evaluate the trained model\n",
        "# Note: The following evaluation is a placeholder and should be replaced with an appropriate evaluation method\n",
        "# based on the specific requirements of your use case. Below is an example evaluation using accuracy.\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(model, test_images, test_labels):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in zip(test_images, test_labels):\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "            labels = labels.unsqueeze(0)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Example evaluation\n",
        "galaxy_cnn = GalaxyCNN()\n",
        "# Assuming that you have trained your galaxy_cnn model\n",
        "test_accuracy = evaluate_model(galaxy_cnn, test_images, test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy}%\")\n",
        "\n",
        "# Model evaluation for the Quantum Policy\n",
        "def evaluate_quantum_policy(actor, critic, n_tests=10):\n",
        "    actor.eval()\n",
        "    critic.eval()\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_tests):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            rewards.append(reward)\n",
        "\n",
        "    avg_reward = sum(rewards) / n_tests\n",
        "    return avg_reward\n",
        "\n",
        "# Example evaluation\n",
        "avg_reward = evaluate_quantum_policy(actor, critic)\n",
        "print(f\"Average Reward: {avg_reward}\")\n",
        "\n",
        "# Combining everything\n",
        "if __name__ == \"__main__\":\n",
        "    # Data preparation\n",
        "    train_dataset = GalaxyDataset(train_images, train_labels, transform=tensor_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    test_dataset = GalaxyDataset(test_images, test_labels, transform=None)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Training and evaluation\n",
        "    galaxy_cnn = GalaxyCNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(galaxy_cnn.parameters(), lr=0.001)\n",
        "\n",
        "    # Train Galaxy CNN\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        galaxy_cnn.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = galaxy_cnn(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate Galaxy CNN\n",
        "    test_accuracy = evaluate_model(galaxy_cnn, test_images, test_labels)\n",
        "    print(f\"Test Accuracy: {test_accuracy}%\")\n",
        "\n",
        "    # Quantum Policy training with best hyperparameters (continued)\n",
        "    actor = QuantumPolicy(n_qubits, n_layers)\n",
        "    critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=best_actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=best_critic_lr)\n",
        "\n",
        "    # Further training as described earlier\n",
        "    # Evaluate Quantum Policy\n",
        "    avg_reward = evaluate_quantum_policy(actor, critic)\n",
        "    print(f\"Average Reward: {avg_reward}\")"
      ],
      "metadata": {
        "id": "Lk29ZwEOaHlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9ApkDdfcZyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms import functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import random\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "# Placeholder for loading dataset\n",
        "train_images = torch.randn(100, 3, 64, 64)  # Example: 100 images, 3 channels, 64x64 resolution\n",
        "train_labels = torch.randint(0, 5, (100,))  # Example: 100 labels, 5 classes\n",
        "test_images = torch.randn(20, 3, 64, 64)   # Example: 20 test images\n",
        "test_labels = torch.randint(0, 5, (20,))   # Example: 20 test labels\n",
        "\n",
        "# Custom transform function\n",
        "def tensor_transform(image):\n",
        "    image = F.rotate(image, angle=30)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.hflip(image)\n",
        "    if torch.rand(1) < 0.5:\n",
        "        image = F.vflip(image)\n",
        "    return F.resized_crop(image, top=0, left=0, height=64, width=64, size=(64, 64))\n",
        "\n",
        "# Custom Dataset class\n",
        "class GalaxyDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Improved CNN with Pre-trained Model\n",
        "class GalaxyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GalaxyCNN, self).__init__()\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)  # Assuming 5 classes\n",
        "        for param in self.resnet.parameters():  # Freeze layers\n",
        "            param.requires_grad = False\n",
        "        for param in self.resnet.fc.parameters():  # Train only the final layers\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Quantum Policy for Reinforcement Learning\n",
        "n_qubits = 4\n",
        "n_layers = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "class CriticNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(CriticNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc2(x[:, -1, :])\n",
        "\n",
        "# Hyperparameter tuning with Optuna\n",
        "def objective(trial):\n",
        "    actor_lr = trial.suggest_float('actor_lr', 1e-5, 1e-2, log=True)\n",
        "    critic_lr = trial.suggest_float('critic_lr', 1e-5, 1e-2, log=True)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "    n_epochs = 20\n",
        "    gamma = 0.99\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rewards, log_probs, state_values = [], [], []\n",
        "        for _ in range(10):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            value = critic(state).squeeze()\n",
        "            state_values.append(value)\n",
        "            log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "            rewards.append(reward)\n",
        "\n",
        "        discounted_rewards = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "        policy_loss, value_loss = [], []\n",
        "        for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "            value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "        total_loss += torch.stack(policy_loss).sum().item() + torch.stack(value_loss).sum().item()\n",
        "\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        value_loss = torch.stack(value_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        value_loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    return total_loss / n_epochs\n",
        "\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "best_actor_lr = study.best_params['actor_lr']\n",
        "best_critic_lr = study.best_params['critic_lr']\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=best_actor_lr)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=best_critic_lr)\n",
        "\n",
        "# Further Training with Best Hyperparameters\n",
        "n_epochs = 50  # Increase the number of epochs for final training\n",
        "gamma = 0.99\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    rewards, log_probs, state_values = [], [], []\n",
        "    for _ in range(10):\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "        policy_output = actor(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "        reward = random.uniform(-1, 1)\n",
        "        value = critic(state).squeeze()\n",
        "        state_values.append(value)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "        rewards.append(reward)\n",
        "\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    policy_loss, value_loss = [], []\n",
        "    for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "        advantage = reward - value.item()\n",
        "        policy_loss.append(-log_prob * advantage)\n",
        "        value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    value_loss = torch.stack(value_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    value_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} - Policy Loss: {policy_loss.item()} - Value Loss: {value_loss.item()}\")\n",
        "\n",
        "# Evaluate the trained model\n",
        "# Note: The following evaluation is a placeholder and should be replaced with an appropriate evaluation method\n",
        "# based on the specific requirements of your use case. Below is an example evaluation using accuracy.\n",
        "\n",
        "# Evaluate Galaxy CNN model\n",
        "def evaluate_model(model, test_images, test_labels):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in zip(test_images, test_labels):\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "            labels = labels.unsqueeze(0)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Example evaluation for Galaxy CNN\n",
        "galaxy_cnn = GalaxyCNN()\n",
        "# Assuming that you have trained your galaxy_cnn model\n",
        "test_accuracy = evaluate_model(galaxy_cnn, test_images, test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy}%\")\n",
        "\n",
        "# Model evaluation for the Quantum Policy\n",
        "def evaluate_quantum_policy(actor, critic, n_tests=10):\n",
        "    actor.eval()\n",
        "    critic.eval()\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_tests):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            rewards.append(reward)\n",
        "\n",
        "    avg_reward = sum(rewards) / n_tests\n",
        "    return avg_reward\n",
        "\n",
        "# Example evaluation for Quantum Policy\n",
        "avg_reward = evaluate_quantum_policy(actor, critic)\n",
        "print(f\"Average Reward: {avg_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Data preparation\n",
        "    train_dataset = GalaxyDataset(train_images, train_labels, transform=tensor_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    test_dataset = GalaxyDataset(test_images, test_labels, transform=None)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Training and evaluation\n",
        "    galaxy_cnn = GalaxyCNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(galaxy_cnn.parameters(), lr=0.001)\n",
        "\n",
        "    # Train Galaxy CNN\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        galaxy_cnn.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = galaxy_cnn(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluate Galaxy CNN\n",
        "    test_accuracy = evaluate_model(galaxy_cnn, test_images, test_labels)\n",
        "    print(f\"Test Accuracy: {test_accuracy}%\")\n",
        "\n",
        "    # Quantum Policy training with best hyperparameters (continued)\n",
        "    actor = QuantumPolicy(n_qubits, n_layers)\n",
        "    critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=best_actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=best_critic_lr)\n",
        "\n",
        "    # Further training as described earlier\n",
        "    for epoch in range(n_epochs):\n",
        "        rewards, log_probs, state_values = [], [], []\n",
        "        for _ in range(10):\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()\n",
        "            reward = random.uniform(-1, 1)\n",
        "            value = critic(state).squeeze()\n",
        "            state_values.append(value)\n",
        "            log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "            rewards.append(reward)\n",
        "\n",
        "        discounted_rewards = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "        policy_loss, value_loss = [], []\n",
        "        for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "            value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        value_loss = torch.stack(value_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        value_loss.backward()\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Policy Loss: {policy_loss.item()} - Value Loss: {value_loss.item()}\")\n",
        "\n",
        "    # Evaluate Quantum Policy\n",
        "    avg_reward = evaluate_quantum_policy(actor, critic)\n",
        "    print(f\"Average Reward: {avg_reward}\")"
      ],
      "metadata": {
        "id": "rqCzUB94cbHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}